---
layout: '../../../layouts/PostLayout.astro'
title: '生成树学习法概述'
language: 'zh'
description: ''
---

## 什么是生成树学习法

生成树学习法博主提出的一种超高效学习方法，适用于一切记忆不密集的、最低理解难度不高的、细节不繁多的、成系统的知识。基本原理是通过构造生成树，利用树结构压缩记忆量以提速的同时，解决知识间依赖问题以显著降低学习难度。此外，还结合了一些认知科学的高效方法，以最大化效率。

这里以学习生成树学习法的学习过程为例，说明如何实现生成树学习法。

## 基础认知

首先从名字开始，需要了解什么是生成树，然后了解各种学习方法的共同点，从而至少能够猜测生成树学习法大概是什么。在完成基础认知之后，你将能够看懂上面那段简介。

### 生成树

在计算机科学和图论中，生成树（Spanning Tree）是指一个无向图中的一个子图，它包含图中的所有顶点，并且是一棵树。也就是说，生成树是原图的一个极小连通子图，它包含原图中尽可能少的边，使得从图中的任何一个顶点都能够到达其他任何一个顶点。

如果原图是连通的，那么它至少有一棵生成树。

生成树的例子包括：

1. 最小生成树（Minimum Spanning Tree, MST）：在一个加权连通图中，最小生成树是所有生成树中边的权重之和最小的那一个。典型的最小生成树算法有Prim算法和Kruskal算法。

2. 最大生成树（Maximum Spanning Tree）：与最小生成树相对，它的边的权重之和最大。

如果你学过一些数据结构方面的知识，我认为你应该是能看懂上面那段话的，但如果你没有学过，不懂的地方很可能是：

1. 图论
2. 无向图
3. 树
4. 极小连通子图
5. 连通
6. Prim算法和Kruskal算法

为了理解什么是生成树，你可能需要学习

1. [图与无向图](#图与无向图)
2. [连通图](#连通图)
3. [树与极小连通子图](#树与极小连通子图)
4. [树的高度与平衡树](#树的高度与平衡树)

你不需要了解Prim算法和Kruskal算法，因为生成树学习法用不到。

那么，为什么要用到生成树呢？

[> 下一节：影响学习效率的因素](#影响学习效率的因素)

#### 图与无向图

图论就是研究图的理论，所以，首先，你需要了解什么是图，什么是无向图。

想象一下你正在上高中，在一个年级中，有很多人是互相认识的。如果我们想在纸上画出谁认识谁，可以用一个点代表每个人，然后如果两个人认识就用线条连接这两个点。

在数学和计算机科学中，这样的画就叫做“图”。图帮助我们用点（称为“顶点”）和线（称为“边”）来表示一些事物之间的关系。顶点就像是网络或者朋友圈中的人，而边则表示它们之间的连接或者关系。

如果用符号表示，一个图$(V,E)$可以表示为顶点的集合$V$加上边的集合$E$。

如果这些线没有方向，比如没有表示谁先认识谁，只要认识了就连上线，或者哪个点是起点哪个是终点，这样的图就叫做“无向图”。在无向图中，边就像是一条没有箭头的线，它只告诉我们两个点是相连的，但不告诉我们连接的方向。

反之，如果存在A认识B但B不认识A，也就是只有A指向B的边，但没有B指向A的边，那么这就是有方向的。这样的图是“有向图”。

[> 返回「生成树」](#生成树)

#### 连通图

沿用上面的比喻，想象一下你和你的同学们在学校里，你们中的每个人都是图中的一个点，如果你和你的某个同学是好朋友，我们就可以在你们两个人之间连上一条边。

一般来说，从一个同学出发，找到他的朋友，再从他的朋友找到他的朋友的朋友，经过若干步后，最终都能到达任何一个同学。如果满足这样的条件，那么我们就可以说这个图是连通图。

反之，如果因为班级间沟通很少，不存在跨班的朋友，那么就不能从一个班的某个同学出发，沿着朋友关系，找到其他班的同学。这样的图就是非连通图。

如果你想学会一个东西，并且这个东西有很多小知识点。那么，显而易见，根据知识点间的关联关系画边，得到的图一定是连通图。从任何一个知识点出发，经过关联关系组成的边，总能找到任何一个其他的知识点。

[> 返回「生成树」](#生成树)

#### 树与极小连通子图

树是一种不同于图的数据结构。

想象一下有一颗倒挂的小树。这棵树有一个根，就像真正的树一样，但在数据结构的树里，根是在最上面的，它就像是树的起点。从这个根部，有好几个分支伸出来，这些分支可以继续分出更多的小分支。

在树中，每一个分支的末端都是叶子。在这个比喻中，叶子和树枝分叉处都代表了一块信息，我们叫它“节点”。每个节点可以有零个、一个或多个子节点。这意味着有些节点下面可能还挂着叶子，而有些节点下面则什么也没有。没有子节点的节点就是叶子。

那么，这棵数据结构的树有什么用呢？想象一下，如果我们把每个节点看成一个家庭成员的名字。树根是“祖父”，他的分支是他的孩子，也就是“父亲”和“母亲”，他们的分支又是他们的孩子，也就是“孙子”和“孙女”。通过这棵树，我们可以清晰地看到谁是谁的父母、祖父母或子女，也就是家族关系。

在计算机科学中，这样的树状结构让我们能够有效地组织和处理数据。比如，它可以帮助我们快速查找信息（比如找到某个人的祖父是谁），或者快速添加和删除信息（比如家族中增加了一个新成员）。

子图是从原来的图中选出一部分组成的图，这个是容易理解的。[连通性](#连通图)上面也有讲。那么，极小的含义是什么？

学过数学分析基础的同学应该熟悉上确界和下确界的定义，极小连通子图的「极小」类似于那种定义方法。一旦删除极小连通子图的任何一条边，那么这个图将不再连通。

[> 返回「生成树」](#生成树)

#### 树的高度与平衡树

树的高度是指从树的根节点到最远叶子节点的最长路径上的节点数。在计算树的高度时，通常将根节点的高度视为1。因此，只有根节点的树高度为1，空树（无节点）的高度为0。

树的高度对某些操作的时间花费影响很大，例如二叉搜索树的查找。

二叉搜索树是一种特殊的树，每一个节点最多只有2个子节点，每个节点都带有一个数。对于任何一个节点，它左边的子节点（如果有）的数比它小，它右边的子节点（如果有）的数比它大。

当我们要判断一个数是否在这个二叉搜索树时，会从根节点开始。如果我们要找的数比根节点小，那么就找左子节点，反之找右子节点。这样一路找下去，要么找到底也没找到，要么找到一个节点的数和我们要找的数相等。所以，最差情况下，需要比较的次数恰好就是树的高度。

假设一共有$n$个节点，最坏情况下，树的高度是$n$，此时每个节点都没有右子节点或者每个节点都没有左子节点。而比较好的情况下，树的高度近似于$\log_2(n)$，此时没有特别长的树枝。

为了解决二叉搜索树在最坏情况下退化成链表的问题，从而导致操作的效率降低，平衡树的概念被引入。平衡树是一种特殊的二叉搜索树，它在保持二叉搜索树所有特性的同时，还增加了一个关键的特性：任何时候任何一个节点的两个子树的高度差都不会超过1。这个特性确保了树的高度大致保持在$\log_2(n)$的数量级，从而使得所有的基本操作（如插入、删除、查找）的时间复杂度都能保持在对数级别。

[> 返回「生成树」](#生成树)

### 影响学习效率的因素

一个看似显而易见但实际上很少有被考虑到的事实是：学习不同种类的知识所适合的方法是不同的。即使是泛用性很强的生成树学习法，也不适用与记忆密集、最低理解难度高、细节繁多、过于杂乱的知识。

#### 记忆密集性

生成树学习法不适用于学习记忆密集的知识，那么，什么是记忆密集的知识？

最典型的例子是高级编程语言，比如Rust。Rust基础是记忆密集的，为了看懂一行Rust代码，你需要记忆大量语法。生成树学习法本身也是有些记忆密集的，因为生成树学习法用到了非常多的数据结构方面的优化。

记忆密集性定义为学习一个知识所需要的依赖的数量。例如，为了学习生成树学习法，你需要理解生成树学习法的名称是什么含义，原理是什么，以及如何运用。这里，生成树学习法的记忆密集性比较低。但是，为了理解生成树学习法的原理，你需要学习图、树等数据结构方面的知识，还需要明白时间复杂度、遍历等算法方面的知识，还需要一些认知科学、数据压缩、信息论方面的知识。这里，生成树学习法的原理的记忆密集型就比较高。

#### 无交集数量

事实上，生成树学习法基本只适用于学习同一类知识。假设学习一门知识所需要的时间为$f(x)$，学习基本不相关的两类知识所需要的时间是$f(a)+f(b)$而非$f(a+b)$。因为生成树学习法的高效，$f(a)+f(b)>f(a+b)$。

无法合并的类的数量会很大程度影响生成树学习法的速度。事实上，大学物理这门课就是典型的有大量无法合并的类的知识集合。大学物理涉及力学、电学、光学、场论基础、量子力学基础等等。力学和电学之间基本没有交集，所以生成树学习法不能将大学物理合并到一起学习，只能分开成各部分分别学习。

#### 细节量

需要记忆的细节量很大程度上影响了学习的效率。需要准确记忆大量细节所花费的时间的科目，显然比不需要准确记忆大量细节的科目所花费的时间多得多。在生成树学习法中，细节量体现为树的高度。

一个典型的需要完整记忆细节的知识是英语单词。你可以把一个英语单词按照构词法和语言学拆分成各个小部分，让大量单词组成可用于生成树学习法的树结构。但是，你必须要记下来整个树才能背下这些单词。

#### 最低理解难度

当一个人已经掌握了学会一个东西所需要学的所有前置知识，以至于学会这个东西就差临门一脚时，理解这个东西所需要的难度就是最低理解难度。对于大多数知识来说，最低理解难度是很低的，以至于大多数情况只要依赖足够完善，学习看上去比较难懂的东西实际上并不需要多高的智商。

但是，确实有很多情况会因为最低理解难度大于理解能力而学不会。比如说，费马大定理的证明的最低理解难度就比较高，很多人无法学会理解证明所需要的几何代数知识，因为他们理解能力不够。

生成树学习法解决了依赖问题，使得理解难度接近最低理解难度，因此能够更快地理解知识点。

#### 学习效率的上限

确实存在学习效率的上限，无论任何学习方法都无法突破这个上限。学习效率的上限为：以最快的速度学会需要用到的全部知识。

首先，速度最快本身就有上限的意义。然后，如果不能学会需要用到的某一个知识点，那么这个学习就是不能满足需要的，可以认为是失败的。

生成树学习法相当接近这个上限。生成树学习法允许忽略细节，使得学到的几乎等同于需要用到的。同时，生成树学习法的天然熵编码和平衡树数据结构使得记忆量被很大限度地压缩了。并且，因为生成树学习法是渐进的、启发式的，所以比较符合人类认知规律，能比较高效的进行记忆且不容易遗忘。

## 生成树学习法的原理

生成树学习法优化了学习过程，并且提供了一种高效的对知识点编码记忆的方法。生成树学习法有自底而上和自顶而下两种类型，它们的大致原理是相同的，但在顺序上有很大不同。

### 学习顺序

以依赖关系构建无向图和生成树，并在树上研究自学的学习轨迹，会发现一些有趣的事情。

我们自学一个东西时，往往有两种情况

+ 为了完成某个任务，学习完成这个任务所需要的知识
+ 从一个基础的起点开始，探索地学完这个领域的各种知识

第一种的例子有：为了做一个电磁炮，学习电磁场、电路相关知识，逐步了解做出电磁炮所需要的全部细节。

第二种的例子有：从集合与映射开始，到勒贝格测度，再到积分论，学完实分析的各种知识。

第一种情况对应的是自顶而下生成树学习法，第二种情况对应的是自底而上生成树学习法。

从树的角度考虑，自顶而下生成树学习法是对生成树的先序遍历，而自底而上生成树学习法是对生成树的后序遍历。

[> 下一节：学习过程](#学习过程)

#### 先序遍历和后序遍历

先序遍历（Pre-order Traversal）和后序遍历（Post-order Traversal）是在树结构中访问和检查节点的两种方式。

1. **先序遍历（Pre-order Traversal）**:
   - 在先序遍历中，我们按照“根节点 - 左子树 - 右子树”的顺序访问树中的每个节点。
   - 先访问根节点，然后递归地对左子树进行先序遍历，最后递归地对右子树进行先序遍历。
   - 举个例子，假设有一棵树，其根节点是1，左子节点是2，右子节点是3。那么先序遍历的顺序就是1 -> 2 -> 3。

2. **后序遍历（Post-order Traversal）**:
   - 在后序遍历中，我们按照“左子树 - 右子树 - 根节点”的顺序访问树中的每个节点。
   - 先递归地对左子树进行后序遍历，然后递归地对右子树进行后序遍历，最后访问根节点。
   - 使用同样的树作为例子（根节点是1，左子节点是2，右子节点是3），后序遍历的顺序就是2 -> 3 -> 1。

### 时间复杂度

时间复杂度是一个衡量算法执行时间长短的标准，通常用来评估一个算法随着输入数据量的增加，其运行时间增长的快慢。它不是直接测量秒数，而是看算法需要执行的步骤数如何随着输入数据的大小而变化。

比如，假设有一个算法，它处理10个数据需要10步，处理100个数据需要100步，那么我们说这个算法的时间复杂度是线性的，通常表示为O(n)，其中n代表输入数据的数量。这意味着数据量加倍时，运行时间也大约加倍。

不同的算法可能有不同的时间复杂度，如O(1)（常数时间，无论数据多少，所需时间都差不多），O(log n)（对数时间，数据量加倍时，所需时间增加的不多），O(n²)（二次时间，数据量加倍时，所需时间增加四倍），等等。通常来说，时间复杂度低的算法比时间复杂度高的算法更有效率。

### 学习过程

#### 自顶而下学习过程

在自顶而下生成树学习法中，学习过程是对子节点的递归解析，同时也是对生成树的先序遍历。

以学习如何制作电磁炮为例，首先，你会想知道电磁炮如何驱动炮弹发射。你了解到，电磁炮主要分为电磁轨道炮（rail gun）和电磁线圈炮(coil gun)。电磁轨道炮使用大电流的安培力驱动炮弹发射，而电磁线圈炮使用电磁铁的磁力驱动铁磁性炮弹发射。

这个过程，就是对根节点「制作电磁炮」进行解析，得到了「电磁炮类型」和「驱动原理」这两个子节点。并且，进行了对「电磁炮类型」的解析。

经过进一步了解之后，你知道电磁轨道炮需要大电流才能发射，而且会因为摩擦力和焦耳热失去大量的热量，因此效率较低。而且，产生大电流也不容易，所以你选择研究实现电磁线圈炮的原理。

这个过程，就是对节点「驱动原理」进行解析，得到「电流驱动」和「磁力驱动」，并**忽略了**「电流驱动」。

接下来，你研究电磁驱动的原理，知道电磁驱动就是用强力的电磁铁吸引铁磁性炮弹运动，然后在炮弹将要离开线圈时关闭电磁铁来防止电磁铁的吸力把炮弹吸回来。为了提高电磁炮的威力，你明白你需要多个线圈逐级加速，需要产生大电流来实现强大的电磁铁，需要控制电路。

这个过程，就是对节点「磁力驱动」进行解析，得到几个子节点。之后你递归地解析这几个子节点，不断学习知识，直到掌握足够制作电磁炮的知识。

设最大细节为$D$，记忆密集度为$m$，实际记忆细节为$d$，那么自顶而下生成树学习法的时间复杂度是
$$
O(dm^D)
$$
计算过程如下：

在探索需要记下来什么的过程中，有：

对于知识总量为$m^D$的知识，解析其根节点，花费的时间为$O(1)$（不考虑难度变化）。

解析出$m$个，知识总量为$m^{D-1}=m^D/m$的子节点。接下来，你解析这些子节点，并逐步合并到你已经学过的知识当中，需要合并$m$次，每次合并所需要的时间取决于记忆量。（实际上，解析子节点和解析父节点是同时进行的，这里分出先后顺序是为了方便计算。）

设学习时间为$T(m^d)$，那么
$$
T(m^D)=mT(m^D/m)+mO(?)+O(1)
$$
因为生成树学习法的过程，你可以很轻松地知道子知识点位于依赖关系的上游还是下游。并且，在合并知识时，原有的知识结构思路大概被原样保留。所以，合并知识点的过程可以看作按秩合并的并查集的合并操作。

（关于并查集，参考[并查集](#并查集)小节）

填写问号处，得到
$$
T(m^D)=mT(m^D/m)+mO(\lg m^{d-1})+ O(1)
$$
当$D$比较小时，可以视为$\lg m^{d-1}\sim m^{d-1}$，应用主定理（master theorem），得到
$$
O(dm^D)
$$

> 按照时间复杂度理论，$\lg m^{d-1}$应该近似于$1$而不是$m^{d-1}$。但是，能够按秩合并的前提条件是对知识结构了如指掌。真实的学习过程中，总会对一些部分不熟悉甚至需要复习，所以以$m^{d-1}$为近似。

#### 自底而上学习过程

自底而上生成树学习法是对生成树的后序遍历。不同于自顶而下学习过程，自底而上需要摸索兄弟节点。



#### 有指导的学习过程

生成树学习法一般用于自学，但是，在具有理想的导师可以依赖时，确实存在比自顶而下和自底而上更快的学习过程。



## 生成树学习法的操作流程

### 准备资料

### 确定学习过程类型

### 自顶而下：弄明白需要什么

### 自顶而下：弄清楚你需要的是什么

### 自顶而下：整理资料

### 自顶而下：递归

### 自底而上：找到起点

### 自底而上：你学的有什么用

### 自底而上：为了实现这个，还需要什么

### 自底而上：有没有其他解决方案

### 自底而上：复习，然后再迈出一步

### 指导：快速复习的艺术
